# 线性回归

## 概率密度函数推导

$$
1.目标值与误差的关系(x是特征向量)\\
y^{(i)} = \theta^T x^{(i)} + \varepsilon^{(i)}\\
2.误差独立同分布，其概率服从标准正态分布(均值为0且方差为\sigma^2)\\
由标准正态分布公式f(x) = \frac{1}{\sqrt{2\pi}\sigma} \exp\left(-\frac{x^2}{2\sigma^2}\right)\\
得到p(\varepsilon^{(i)}) = \frac{1}{\sqrt{2\pi}\sigma} \exp\left( -\frac{(\varepsilon^{(i)})^2}{2\sigma^2} \right)\\
3.由1、2得在给定x^{(i)}与\theta时目标值y^{(i)}的条件概率服从方差为\sigma^2且均值为\theta^T x^{(i)}的正态分布\\
p(y^{(i)} | x^{(i)}; \theta) = \frac{1}{\sqrt{2\pi\sigma}} \exp \left( -\frac{(y^{(i)} - \theta^Tx^{(i)})^2}{2\sigma^2} \right)
$$

## 最大似然值推导（最小二乘法）

$$
想象一下，你有一套模型参数\theta，你想知道在这些参数下，你观测到数据点\{{x^{(i)},y^{(i)}}\}有多么可能。\\
如果你有多个数据点，你需要计算每个数据点在模型中的可能性，然后把它们综合起来，来衡量这些参数是否适合所有数据点。\\
由于独立性的假设，这些概率可以通过相乘来组合，乘积被叫做联合概率，此时有关\theta的函数被叫做似然函数，用来衡量某个参数的拟合程度\\
通过求似然函数的极值，便能找到最优参数\\
1.由上，似然函数理解为特定\theta下所有\{{x^{(i)},y^{(i)}}\}概率的乘积\\
L(\theta)=\prod_{i=1}^{m} \frac{1}{\sqrt{2\pi\sigma}} \exp \left( -\frac{(y^{(i)} - \theta^T x^{(i)})^2}{2\sigma^2} \right)\\
2.对似然函数取对数，化乘法运算为加法运算\\
\begin{aligned}
& \sum_{i=1}^{m} \log \frac{1}{\sqrt{2\pi\sigma}} \exp \left( -\frac{(y^{(i)} - \theta^T x^{(i)})^2}{2\sigma^2} \right) \\
= & \  m \log \frac{1}{\sqrt{2\pi\sigma}}  -  \frac{1}{\sigma^2} \cdot \frac{1}{2} \sum_{i=1}^{m} (y^{(i)} - \theta^T x^{(i)})^2.
\end{aligned}\\
3.对后半段部分\frac{1}{2}\sum_{i=1}^{m} (y^{(i)} - \theta^T x^{(i)})^2进行矩阵化方便计算\\
得到J(\theta) = \frac{1}{2}(X\theta - y)^T(X\theta - y) \\
4.求偏导\\
\begin{aligned}
\nabla_{\theta} J(\theta) &= \nabla_{\theta} \left[ \frac{1}{2} (X\theta - y)^T (X\theta - y) \right] = \nabla_{\theta} \left[ \frac{1}{2} (\theta^T X^T - y^T)(X\theta - y) \right] \\
&= \nabla_{\theta} \left[ \frac{1}{2} (\theta^T X^T X \theta - \theta^T X^T y - y^T X \theta + y^T y) \right] \\
&= \frac{1}{2} (2X^T X \theta - X^T y - (y^T X)^T) = X^T X \theta - X^T y
\end{aligned}\\
5.令X^T X \theta - X^T y=0求最大似然值点\\
得\theta = (X^T X)^{-1} X^T y
$$

## 梯度下降法

$$
最大似然函数的直接求解涉及到求X^T X的逆矩阵，具有O(n^3)的时间复杂度，其中n是特征数，计算成本高，甚至无解，所以我们需要梯度下降法来求解。\\
线性回归推导出的似然值核心项为\frac{1}{2}\sum_{i=1}^{m} (y^{(i)} - \theta^T x^{(i)})^2，可是为了消除样本数量m的影响，梯度下降的目标函数还要求个平均：\\
梯度下降法的最终目的也就是对这个目标函数求得一个近似最优值：J(\theta)=\frac{1}{2m}\sum_{i=1}^{m} (y^{(i)} - \theta^T x^{(i)})^2\\

\theta是由多个参数项组成的，但由于特征之间相互独立，所以计算时应该是对每个参数项\theta^{(i)}梯度下降，而非整个\theta下降\\
对\theta^{(i)}求偏导得到：\frac{\partial J(\theta)}{\partial \theta_j} = \frac{1}{2m} \sum_{i=1}^{m} 2(y^{(i)} - \theta^T x^{(i)})(-\frac{\partial}{\partial \theta_j} \theta^T x^{(i)})\\
化简得到：\frac{\partial J(\theta)}{\partial \theta_j} = -\frac{1}{m} \sum_{i=1}^{m} (y^{(i)} - \theta^T x^{(i)}) x_j^{(i)}\\
这就是趋向于新的最优点的梯度，给梯度乘上一个学习率\alpha，所以新参数\theta' _j = \theta _j + \frac{\alpha}{m} \sum_{i=1}^{m} (y^{(i)} - \theta^T x^{(i)}) x_j^{(i)}\\
这是批量梯度下降的公式，迭代速度往往非常慢，于是有了随机梯度下降\theta' _j = \theta _j + \alpha(y^{(i)} - \theta^T x^{(i)}) x_j^{(i)}\\
但是随机梯度下降并不一定每次都沿着收敛的方向，所以就有了小批量梯度下降法：\theta' _j = \theta _j + \frac{\alpha}{10} \sum_{k=i}^{i+9} (y^{(k)} - \theta^T x^{(k)}) x_j^{(k)}\\
上面公式中的10就是一次迭代的样本数，也就是我们常说的batchsize
$$

## 模型评估方法

$$
线性回归模型的评估用到的是统计学中的决定系数R^2，其值由可解释变异(即回归平方和)与总变异(即总平方和)之商得到\\
总平方和：SST = \sum_{i=1}^{n} (y_i - \bar{y})^2，即实际值与平均值差的平方和\\
回归平方和：SSR = \sum_{i=1}^{n} (\hat{y_i} - \bar{y})^2 ，即预测值与平均值差的平方和\\
残差平方和：SSE = \sum_{i=1}^{n} (\hat{y_i} - y_i)^2 ，即预测值与实际值差的平方和\\
最后决定系数有两种表示方式:\\
R^2 = \frac{SSR}{SST} = \frac{\sum_{i=1}^{n} (\hat{y_i} - \bar{y})^2}{\sum_{i=1}^{n} (y_i - \bar{y})^2}\\
或R^2 = 1 -\frac{SSE}{SST} = 1 - \frac{\sum_{i=1}^{n} (\hat{y_i} - y_i)^2}{\sum_{i=1}^{n} (y_i - \bar{y})^2}\\
还有其他的标准:\\
均方误差：MSE = \frac{SSE}{n} = \sum_{i=1}^{n} \frac{1}{n} (\hat{y_i} - y_i)^2\\
均方根误差：RMSE = \sqrt {MSE
} = \sqrt {\sum_{i=1}^{n}\frac{1}{n} (\hat{y_i} - y_i)^2}\\
平均绝对误差: MAE = \text{MAE} = \frac{1}{n} \sum_{i=1}^{n} |y_i - \hat{y}_i|\\
如果RMSE的值是0.25，意味着平均而言，每个预测值与实际值的偏差约为0.25个目标变量单位\\
MAE与RMSE单位相同，RMSE对异常值的敏感性更高，更容易被离群点影响，比MAE直观
$$

## 非线性回归

$$
非线性回归与线性回归分析的主要区别在于，非线性回归模型中的自变量与因变量之间的关系不是线性的，而是遵循某种非线性函数形式。\\
即公式不再是：\\
y^{(i)} = \theta^T x^{(i)} + \varepsilon^{(i)}\\
而是类如以下的：\\
y^{(i)} = \theta_0 + \theta_1 x^{(i)} + \theta_2 (x^{(i)})^2 + \theta_3 (x^{(i)})^3 + \varepsilon^{(i)}\\
y^{(i)} = \theta_0 e^{\theta_1 x^{(i)}} + \varepsilon^{(i)}\\
y^{(i)} = \theta_0 + \theta_1 \ln(x^{(i)}) + \varepsilon^{(i)}\\
y^{(i)} = \frac{1}{1 + e^{-(\theta_0 + \theta_1 x^{(i)})}}\\
y^{(i)} = \theta_0 e^{-\frac{(x^{(i)} - \theta_1)^2}{2\theta_2^2}} + \varepsilon^{(i)}\\
其中的\theta往往都是不同的参数向量，它们的维度与x的维度相当
$$

## 标准化

$$
为什么要在数据预处理时标准化：\\
1.如果直接用原始指标值进行分析，就会突出数值较高的指标，削弱数值水平低指标的作用；标准化处理后，各指标值处于同一数量级别，便于分析。\\
2.梯度下降法时，标准化后可以加快梯度下降的求解速度，即提升模型的收敛速度，如下图所示，正圆下降速度显然更快。\\
标准化的方法：原始数据减均值之后，再除以标准差。将数据变换为均值为0，标准差为1的分布\\
公式： [ X_{New} = \frac{X - \mu}{\sigma} ]，其中求标准差：\sigma = \sqrt{\frac{1}{N} \sum_{i=1}^{N} (x_i - \mu)^2} \\
$$

<img src="https://s2.loli.net/2024/12/21/Hiq3fYGXMzbvy2j.png" alt="st2.png" style="zoom: 80%;" /><img src="https://s2.loli.net/2024/12/21/d6GE7XgzUJ8KZMN.png" alt="st1.png" style="zoom: 80%;" />

## 正则化

$$
为什么要在目标函数中使用正则化,先来说说过拟合：\\
过拟合原因：1.数据噪声太大；2.特征太多；3.模型太复杂;\\
过拟合的解决办法：1.清洗数据;2.减少模型参数,降低模型复杂度;3.正则化,减少参数的大小。\\
如何做到正则化：\\
1.L1正则化，即Lasso回归，公式J(\theta) = MSE(\theta) + \alpha\sum_{i=1}^{N}\abs{\theta_{i}}\\
2.L2正则化，即Ridge(岭)回归，公式J(\theta) = MSE(\theta) + \alpha\sum_{i=1}^{N}\theta_{i}^2\\
其实正则项本质是个惩罚因子，将参数绝对值和或平方和掺到代价函数中，可以有效抑制过大的参数\\
两种正则化的区别：\\
L2对绝对值大的权重予以重惩，当权重绝对值趋近于0时基本不惩罚。即越大的数，其平方越大，越小的数，比如小于1的数，其平方反而越小；\\
L1正则化对所有权重予以同样惩罚，较小的权重在被惩罚后就会变0。模型训练好后，这些权值等于0的特征可以省去，从而达到模型稀疏化的效果。\\
原因,按求导直观来看：
\\L2求\theta^2的梯度为2\theta，它的减小速率随着自身减小而不断减小，所以永远趋近于零而不会到达零；\\
而L1的梯度则为1，可以理解为减小速率始终是个常数，最终会减到0\\
L2更适合防止过拟合，使网络更倾向于使用所有输入特征，而不是严重依赖输入特征中某些小部分特征；L1则更适合特征选择\\
$$

# 逻辑回归

## 逻辑回归概率密度函数推导

$$
对于经典二分类问题，目标值y只有0与1，而回归本身的预测结果并非是两个值\\
所以我们可以把预测结果映射到[0,1]这个区间内，成为一个置信度，或者说概率；\\
如推断出图片中的动物是猫的置信度为0.8，也就是说图中动物有8成概率是猫，就可以认为归到了猫类。\\
把预测的结果映射到[0,1]内可以通过sigmoid函数g(z) = \frac{1}{1 + e^{-z}}\\
令h_{\theta}(x) = g(\theta^T X)，在给定x与\theta时y的概率密度函数就是\\
p(y | x; \theta) = (h_{\theta}(x))^y(1-h_{\theta}(x))^{1-y}\\
同样举个例子，可以认为h_{\theta}(x)是把x分到猫类的概率，类别1为猫类；那么类别0就是狗类，1-h_{\theta}(x)就是分到狗类的概率
$$

## 最大似然与梯度下降

$$
同线性回归最大似然值如何推导那般，我们可以得到\\
L(\theta)=\prod_{i=1}^{m}  (h_{\theta}(x_i))^{y_i}(1-h_{\theta}(x_i))^{1-y_i}\\
取对数得到：
\sum_{i=1}^{m} (y_i\log{h_{\theta}(x_i)} + (1-y_i)\log{(1-h_{\theta}(x_i)})\\
以上就是最大似然值，我们要求它的最大值；先求个平均再取反方便梯度下降\\
得到代价函数：J(\theta) = - \frac{1}{m} \sum_{i=1}^{m} (y_i\log{h_{\theta}(x_i)} + (1-y_i)\log{(1-h_{\theta}(x_i)})\\
进行梯度下降，求偏导：\\
\frac{\delta}{\delta\theta_j} J(\theta) = -\frac{1}{m} \sum_{i=1}^m \left[ y_i \frac{1}{h_\theta(x_i)} \frac{\delta}{\delta\theta_j} h_\theta(x_i) - (1-y_i) \frac{1}{1-h_\theta(x_i)} \frac{\delta}{\delta\theta_j} h_\theta(x_i) \right]\\
= -\frac{1}{m} \sum_{i=1}^m \left[ y_i \frac{1}{g(\theta^\top x_i)} - (1-y_i) \frac{1}{1-g(\theta^\top x_i)} \right] \frac{\delta^*}{\delta\theta_j} g(\theta^\top x_i)\\
= -\frac{1}{m} \sum_{i=1}^m \left[ y_i \frac{1}{g(\theta^\top x_i)} - (1-y_i) \frac{1}{1-g(\theta^\top x_i)} \right] g(\theta^\top x_i) (1-g(\theta^\top x_i)) \frac{\delta}{\delta\theta_j} \theta^\top x_i\\
= -\frac{1}{m} \sum_{i=1}^m (y_i (1-g(\theta^\top x_i)) - (1-y_i)g(\theta^\top x_i)) x_i^j\\
= -\frac{1}{m} \sum_{i=1}^m (y_i - g(\theta^\top x_i)) x_i^j\\
所以新参数\theta' _j = \theta _j + \frac{\alpha}{m} \sum_{i=1}^{m} (y_i - g(\theta^T x_i)) x_j^{(i)}\\
$$

## 多分类逻辑回归sigmoid

$$
当如上方使用sigmoid函数进行所谓的多分类逻辑回归，不过是进行多次二分类逻辑回归\\
举个例子，要将桌面上一堆不同的小球分为红黄蓝三类\\
应当先进行一次红色，非红色的双分类逻辑回归，计算出是红色类别的概率，然后再针对蓝色，黄色都进行一次\\
得到诸如[0.7,0.4,0.3]的结果，这表示着小球是红色的\\
假如数据样本为100*6，总共有3类别，我们想要的结果应该是100*3，所以参数应该是6*3，这是多分类逻辑回归时矩阵大小一般的设定\\
其评估系数一般为在测试集上准确率，其损失一般使用交叉熵(见下方详述)
$$

## 信息熵与交叉熵

$$
进制编码往往能传递比其本身更多的信息。一般来说，对于一个长度为 n 的二进制编码，其所能表达的可能事件数量为2^n。\\
信息论的创始人香农很早就想到了这点，他将二进制编码的长度称为信息量。一般来说，明确有 N 种可能的事件所需要的信息量为 log_2 N。\\
为了便于理解，我们在均匀分布的假设下延申，对于某个事件发生的概率 p，可以对应的可能事件数量为 N = \frac{1}{p}。\\
再说回信息量公式，则可以推出单个事件的信息量为 -\log_2 p。对于整个系统，需要计算每个事件发生的概率乘以其信息量，即 -p \log_2 p。\\
在一组可能事件中，每个事件发生的概率分布所包含的信息量，称之为信息熵，这就是大名鼎鼎的香农公式：\\
\H = \sum_{i=1}^{n} -p_i \log_2 p_i\\
再说到交叉熵，它常用于衡量两个概率分布之间的距离。它反映了用一个预测分布𝑄来或近似另一个真实分布𝑃时的代价。\\
\H(P,Q) = \sum_{i=1}^{n} -p_i \log_2 q_i\\
交叉熵表示用预测分布𝑄编码真实分布𝑃的平均编码长度。其值越小，说明拟合越好。\\
如果𝑃=𝑄，即预测分布完美等于真实分布，交叉熵等于𝑃的熵;如果Q偏离𝑃,交叉熵会大于真实分布的熵，增加的部分称为相对熵（KL散度）\\
在上方二分类代价函数J(\theta)其实已经使用到了这一点：y_i\log{h_{\theta}(x_i)} + (1-y_i)\log{(1-h_{\theta}(x_i)})
$$

## 多分类逻辑回归softmax

$$
softmax公式本质上是求该类别值在所有类别上的占比，如\frac{a}{a+b+c}\\
由于值本身可能差异可能并不大，导致占比区分不明确，所以我们需要尽可能放大这一差异，于是乎公式就变成了这样：g(z) = \frac{e^z}{\sum_j{e^j}}\\
其代价函数同样使用交叉熵函数J(\theta) = -\frac{1}{m} \sum_{i=1}^{m} \sum_{k=1}^{K} y_k^{(i)} \log \left({g(\theta^TX_k)}\right),k是类别数量\\
同样地求偏导梯度下降计算新参数，得到θ'_j = \theta _j + \frac{\alpha}{m} \sum_{i=1}^{m} (y_i - g(\theta^T x_i)) x_j^{(i)}\\
很显然与sigmoid的梯度下降是一致的，区别仅在于激活函数g的不同
$$

## 两种激活函数的比较

| 特性           | Sigmoid                          | Softmax                            |
| -------------- | -------------------------------- | ---------------------------------- |
| **适用场景**   | 多标签分类，一个数据可以分到多类 | 单标签分类，一个数据只能分到一类   |
| **类别独立性** | 每个类别概率独立预测，互不影响   | 类别间有概率竞争关系，概率总和为 1 |
| **输出范围**   | 每个类别的概率在 [0, 1]          | 概率分布，总和为 1                 |
| **典型任务**   | 图像多标签分类，文本情感分析     | 图像单标签分类（如手写数字分类）   |
