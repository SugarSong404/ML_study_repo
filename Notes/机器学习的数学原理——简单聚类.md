# KMeans聚类

## 介绍

所谓聚类就是在没有标签的情况下分类，KMeans可以算作一种最简单的聚类算法

然而由于其初始质心的随机性**(见工作流程)**，有时得到的可能是局部最优解，且相同数据每次训练的结果都可能不一样,存在着一定局限性

![output.png](https://s2.loli.net/2025/01/03/emoFDCfnPXQUA3S.png)

## 工作流程

1. 首先需要确定最后簇的个数N，即最终要分为几类
2. 随机抽取数据中的N个点，作为初始的簇质心
3. 针对其它的每个样本点，分别找到离它们最近**(最近可以用多种方式来衡量，如余弦最近或者欧式距离)**的簇质心，将它们划分到该簇中
4. 根据每个簇内所有的数据，计算出新的簇质心
5. 重复3、4步骤直到质心的变化小于一定值完成聚类

$$
数据向量间欧氏距离公式：\sqrt {\sum_{i=1}^k(a_i-b_i)^2}，k为特征数
$$

$$
数据的质心：\frac{1}{n}\sum_{i=1}^n{x_i}，n为数据样本数
$$



## 评估方法

使用KMeans算法时，在没有标签的情况下对训练结果进行评估，我们可以使用以下两种方法：

- **Inertia(簇内平方和)** 

Inertia越小，代表着簇内样本越相似，聚类的效果就越好
$$
设簇质心为c，数据样本数为n，其公式为：\sum_{i=1}^n{(x_i-c_i)^2}
$$
其实这个公式很类似于SSE(残差平方和)，只不过将**当前数据与质心之差**看作了**真实值与预测值之差**

将每个簇的Inertia相加就得到了整体平方和，这可以用作对模型整体水平的一个衡量

- **轮廓系数**

$$
公式为：S(i) = \frac{b(i)-a(i)}{max(a(i),b(i))}
$$
$$
其中a(i)为样本i到同簇内其它样本的平均距离，b(i,j)为样本i到其它某簇c_j内样本的平均距离,b(i)是b(i,j)对于所有j中的最小值
$$

若S(i)接近1，说明b(i)远大于a(i)，证明了样本i聚类合理

若S(i)接近0，说明b(i)与a(i)值差不多，证明了样本i在边界点

若S(i)接近-1，说明a(i)远大于b(i)，证明了样本i更应该分到别的簇

针对所有样本的轮廓系数求平均就能得到模型整体水平的一个衡量

# DBSCAN聚类

## 介绍

DBSCAN是另一种常见的聚类方法（Density-Based Spatial Clustering of Applications with Noise，具有噪声的基于密度的聚类方法）

该算法将具有足够密度的区域划分为簇，所以它支持发现任意形状的簇，并且不需要像KMeans一样预设定簇的个数

<img src="https://s2.loli.net/2025/01/03/vyDWk7emGNJH4Lw.png" alt=" 2025-01-03 160445.png" style="zoom:50%;" />

## 工作流程

先说一个概念，对于一个数据点，以其为圆心，半径为`radius`的范围内，如果点数超过`density`，则说明它是一个核心点，否则就是一个离群点

其中`radius`，`density`就是DBSCAN算法的参数

1.首先在样本中随机找到一个核心点，作为一个簇C的第一个点

2.对这个核心点范围r内的所有点进行是否为核心点的判断，如果是就放入簇C中

3.对簇C中的所有点都执行第2步操作，直到不是核心点或者该点已经存在于C中**（其实就是一个生长的操作，在密度小的地方会断掉）**

4.此时簇C已经完成聚类，对剩下的样本重新开始第1步以开始新的簇聚类，以此类推，直到只剩下离群点和已经聚类的点

